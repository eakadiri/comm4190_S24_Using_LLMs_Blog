[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Favour‚Äôs Explorations with LLMs",
    "section": "",
    "text": "Blog Post 2 : Deepfakes in the Music Industry ‚Äî How Much is Too Much?\n\n\n\n\n\n\nAI\n\n\nMusic\n\n\nDeepfakes\n\n\n\nAnalyzing the usage of AI to impersonate musicians.\n\n\n\n\n\nFeb 27, 2024\n\n\nEbunoluwa Akadiri\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 1 : AI in Health Care\n\n\n\n\n\n\nAI\n\n\nHealthcare\n\n\n\nAdvantages and Limitations of AI in Health Care Administration\n\n\n\n\n\nFeb 27, 2024\n\n\nEbunoluwa Akadiri\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/002_BlogPost/BlogPostTwo.html",
    "href": "posts/002_BlogPost/BlogPostTwo.html",
    "title": "Blog Post 2 : Deepfakes in the Music Industry ‚Äî How Much is Too Much?",
    "section": "",
    "text": "Deepfakes in the Music Industry ‚Äî How Much is Too Much?\nMany of us are familiar with celebrity impersonators, people who make livings off their uncanny resemblances to celebrities. A good example of this is Izzy otherwise known as ‚Äúfake Drake‚Äù who built a large following and made up to 5,000 for each event he appears at pretending to be Drake:\n\n\n\nfakedrake.jpeg\n\n\nHowever, AI has led to a new sort of technology, one that can be used to mimic the voices of certain celebrities, namely singers and rappers, on various songs.\nAn example of this is Musicfy‚Äôs Free Celebrity AI Voice Generator Link.\nThis generator claims to have access to the voices of over 3,000 celebrities from Donald Trump to Ariana Grande. The site claims that the ability to listen to any celebrity at any time creates the feeling of a personalized concerts.\nHere are some examples of some famous celebrities performing songs that are not their own:\n\nIf Linkin‚Äô Park Made the Pokemon Theme Song\n\nLink\n\nRihanna Singing Wonderwall by Oasis\n\nLink\n\nRihanna Singing Fast Car by Tracy Chapman\n\nLink\nWhat‚Äôs particularly interesting about this cover is it is distinctively out of key. As a musician, I can discern that she starts off in the correct key but each line after is in a different/wrong key. This makes it a bit easier to identify it as AI.\nAside from the declination in quality, it is also important to consider the ethical dilemma that comes with attaching someone‚Äôs voice to something they didn‚Äôt say, or in this case sing.\nWe‚Äôve seen with political deepfakes how dangerous it can be to create videos of politicians saying or doing things they did not say or do in real life. It leads to an overall decline in quality of political information available and general mistrust of the government. In the case of music, not many labels and agencies have protected rights over their Artificial likeness which is what is allowing these websites to take the voices of celebrities and put them in different songs and contexts.\nIt is important that artists protect their rights, but it is also important to recognize that we as consumers, shoukd try to consume ethical art, not art made artificially."
  },
  {
    "objectID": "posts/001_BlogPost/BlogPostOne.html",
    "href": "posts/001_BlogPost/BlogPostOne.html",
    "title": "Blog Post 1 : AI in Health Care",
    "section": "",
    "text": "AI in Health Care\nIn recent years, there has been an influx of the usage of AI in healthcare administration.\nThe current state of healthcare communication emphasizes immediacy: communication features that promote physical and emotional closeness, authenticity, and enthusiasm. Increasing immediacy equates to increasing the perception of approachability and psychological closeness. In health care, this means a greater and stronger connection between health care professionals and patients (Kreps 2013). However, the current state of communication shows a clear disconnect between consumers and healthcare professionals. Healthcare promotions and providers currently focus more on procedures and medical technology rather than making messaging towards consumers more inviting and less alienating. This is due to a greater focus on profit and advancement rather than solution based medicine that directly targets the needs of consumers and reduces multiple visits and repeated incidences. Additionally, many health care professionals struggle to explain medical concepts to consumers in a clarifying manner which leads to gaps of knowledge.\nLarge Language Models like ChatGPT show potential growth in streamlining patient-doctor communication as well as providing consumers with personalized health education and information. Common applications are diagnosing patients, transcribing documents, treating patients, and many more uses.\nHere are some examples of LLMs currently being used in the medical field:\n1. Use of ChatGPT in Medical Examinations\nIn a study done by the Journal of Medical Systems, they tested GPT-3‚Äôs ability to write a medical note given information regarding treatment, samples, and other health parameters. Even with various abbreviations, it was able to categorize parameters to the right sections. However, a major limitation was its inability to address causal relationships between symptoms. It also was not designed for answering medical questions, lacking expertise to fully understand the relationships between conditions and symptoms. Though it provided some advice, it was very general and it excelled more at summarizing information in both technical and plain language. [https://doi-org.proxy.library.upenn.edu/10.1007/s10916-023-01925-4]\n2. Comparison of ChatGPT‚Äôs Ability to Human HealthCare Professionals\nA study was conducted comparing physician responses to chatbot responses to questions from the reddit forum r/AskDocs. They evaluated which responses were better, quality of information, empathy/bedside manner. It was found that evaluators preferred chatbot responses to physician responses [78.6% of 585 evaluators], evaluators rated on average chatbot responses as good vs.¬†acceptable for physician responses, and evaluators rated ChatGPT to be more empathetic [physician responses were 41% less empathetic]. Even when physician responses were longer and scored higher for quality and empathy, they still remained significantly below chatbot scores. This shows implications for AI as a critical resource for fostering patient equity as individuals with mobility limitations or fear of medical bills are more likely to use messaging. However, this study did not assess neither the physician nor the bot response for fabricated information (Ayers 2023). [https://pubmed.ncbi.nlm.nih.gov/37115527/]\n3. Conversational Artificial Intelligence for Spinal Pain Questionnaire\nEfforts to apply artificial intelligence to medicine are actively underway with the creation and application of AI associated with natural language processing [NLU] However, unlike writing based chatbots like ChatGPT, Spoken Dialog Systems [SDS] are proving to be a possible gamechanger. SDS are able to communicate by voice and maintain conversation over long periods of time. Many companies already use SDS like Amazon with their Alexa, but this study tries to develop an SDS for a pain questionnaire for patients with spinal disease.\nThe questionnaire was developed by inputting questions that medical staff usually ask during rounds of inpatients including questions about location, duration, intensity of the pain. Each question was structured in a closed format to allow for the system to easily answer questions. Real doctor-patient dialogue sets were collected for both the preoperative and postoperative pain questionnaires.\nThe Breakdown:\n\nThe patient‚Äôs voice is entered in a speech recognition module and converted into text data.\nThis data is entered into a natural language processing module and the output of the module is entered into a dialog management module which manages conversation flow\nThe dialogue module then searches for information to be given to the user which is generated as text data and then put back into the speech recognition module to be uttered in a coherent voice format.\nAny input of patients that had to do with the expression of pain/mental state were classified [duration, timing, etc].\nEach patient‚Äôs personal information is assigned an ID number which allows the SDS to check if the patient already exists.\nThe SDS then makes a summary of the questionnaire and if proper information was not obtained, asks the question until the information is obtained.\nWhen all the information is obtained, the SDS makes a summarized result statement of the questionnaire in text data,\n\nOverall, when evaluating the user satisfaction, the average score between doctors, nurses, and patients was 5.5 +- 1.4 / 7 points. There was higher satisfaction in regard to the clarity and positivity of the SDS, but relatively lower scores for the similarity of conversation with real people. In terms of the performance accuracy, the SDS struggled to recognize the answers of patient groups and increased repetition. In terms of errors, there were statistically different numbers of errors between doctors and patients and nurses, with nurses getting 0 errors. [https://doi-org.proxy.library.upenn.edu/10.14245/ns.2143080.540]\nBut, there are some drawbacks associated with this growth:\nA. Explainability and Interpretability\nExplainability refers to the ability of models to justify results, while interpretability refers to the ability of the model to determine causes and effects from inputs and outputs. When considering explainability, models with high explanatory power are able to take into account multiple factors and use each factor to determine an outcome. As an example, when asked to predict when a person might die, a model would take into account factors like age, BMI, years spent smoking, and career and weigh each of those factors as more or less important in dictating a person‚Äôs lifespan. It allows models to know what inputs represent and how important they are to the overall outcome. When models fail to explain outcomes, it simply leads to a lack of accuracy in responses and model malfunctions.\nB. Black Box Effect\nAnother thing to consider is the ‚Äúblack box‚Äù effect which refers to the lack of transparency of the operation of AI systems (Fenster 2023). Thus, the process in which AI models like ChatGPT analyzes is not fully understood by humans leading to undetected errors. Due to its tendency to hallucinate, unless experts in the field of question, many people would struggle to cross-check information provided due to the truthful manner it is provided in. This is especially true because unless prompted, it does not explain or advise on its limitations like the tendency to produce misinformation, bias in training data, or security risks from gathered data. This is a problem that lies mainly with experts designing these algorithms. The decisions for how models interact with users is left to the discretion of data scientists, leading to disconnects in customer desires for usage. Though these challenges are less applicable to medicine due to less data in large quantities especially when compared to fields like Bioinformatics. However, using databases like electronic health records (EHR) and other patient information databases not only bring up matters about data security but also the limited data may create records and solutions that fit modeling needs rather than medical needs (Vellido 2020).\nC. Racial Bias\nAnother major issue lies in the clear clustering of health data available for white people when compared to people of color. This also extends to disparities in both gender and sexual orientation data. Discrimination is a major player in healthcare, and AI can mitigate health-care disparities, but at the same time, it can also exacerbate them. AI. Hospitals are becoming increasingly reliant on AI which is able to analyze and produce potential treatment options; however, there is a lack of consideration about algorithmic discrimination based on pre-existing biases and data in the medical field. Health data that is available can be limited/incorrect across people with different conditions and of different races which can lead to measurement errors and selection bias. It could also be historical patterns of discrimination which can lead to feedback loop bias. In 2019, a study was done to show how an algorithm, created by Optum, used by UnitedHealth Group was discriminating against Black patients. It was found that the algorithm was prioritizing care in hospitals based on healthcare costs and spending patterns, disproportionately targeting Black patients who historically have had lower health care spending. This is namely the only proven discrimination in healthcare, but instances like this are predicted to increase over time. The lack of guidelines and accountability for AI discrimination poses challenges for patients seeking legal recourse (Fenster 2023).\nConclusion:\nThere is still much work to be done to fully understand and control the use of AI in healthcare. In its current form, ChatGPT and other LLMs can only serve as tools alongside knowledgeable healthcare professionals. Though they have already demonstrated their ability to bridge the gap between healthcare consumers and professionals in various ways, they lacked the ability to accurately interpret data a large percentage of the time. Though these applications are efficient, most are time consuming and few have been integrated into clinical practice. More comprehensive research needs to be conducted before the integration of AI in healthcare further. Addressing these challenges through visualization and involving healthcare professionals‚Äô external assessment to ensure models address medical issues, comply with guidelines, and align with system-human interaction is the key to the integration of AI in healthcare (Vellido 2020)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]