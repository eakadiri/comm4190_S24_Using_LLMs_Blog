{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5380f0bb-1960-4258-a4f9-6c4ae3756255",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"AI in Health Care\"\n",
    "description: \"Advantages and Limitations of AI in Health Care Administration\"\n",
    "author: \"Ebunoluwa Akadiri\"\n",
    "date: \"2/27/2024\"\n",
    "categories:\n",
    "  - AI\n",
    "  - Healthcare\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7836a9-d6bf-4d45-8c1f-4b0891393d85",
   "metadata": {},
   "source": [
    "# AI in Health Care\n",
    "\n",
    "In recent years, there has been an influx of the usage of AI in healthcare administration. \n",
    "\n",
    "The current state of healthcare communication emphasizes immediacy: communication features that promote physical and emotional closeness, authenticity, and enthusiasm. Increasing immediacy equates to increasing the perception of approachability and psychological closeness. In health care, this means a greater and stronger connection between health care professionals and patients (Kreps 2013).  However, the current state of communication shows a clear disconnect between consumers and healthcare professionals. Healthcare promotions and providers currently focus more on procedures and medical technology rather than making messaging towards consumers more inviting and less alienating. This is due to a greater focus on profit and advancement rather than solution based medicine that directly targets the needs of consumers and reduces multiple visits and repeated incidences. Additionally, many health care professionals struggle to explain medical concepts to consumers in a clarifying manner which leads to gaps of knowledge.\n",
    "\n",
    "Large Language Models like ChatGPT show potential growth in streamlining patient-doctor communication as well as providing consumers with personalized health education and information. Common applications are diagnosing patients, transcribing documents, treating patients, and many more uses.\n",
    "\n",
    "**Here are some examples of LLMs currently being used in the medical field:**\n",
    "\n",
    "**1. Use of ChatGPT in Medical Examinations**\n",
    "\n",
    "In a study done by the Journal of Medical Systems, they tested GPT-3’s ability to write a medical note given information regarding treatment, samples, and other health parameters. Even with various abbreviations, it was able to categorize parameters to the right sections. However, a major limitation was its inability to address causal relationships between symptoms. It also was not designed for answering medical questions, lacking expertise to fully understand the relationships between conditions and symptoms. Though it provided some advice, it was very general and it excelled more at summarizing information in both technical and plain language. [https://doi-org.proxy.library.upenn.edu/10.1007/s10916-023-01925-4]\n",
    "\n",
    "**2. Comparison of ChatGPT's Ability to Human HealthCare Professionals**\n",
    "   \n",
    "A study was conducted comparing physician responses to chatbot responses to questions from the reddit forum r/AskDocs. They evaluated which responses were better, quality of information, empathy/bedside manner. It was found that evaluators preferred chatbot responses to physician responses [78.6% of 585 evaluators], evaluators rated on average chatbot responses as good vs. acceptable for physician responses, and evaluators rated ChatGPT to be more empathetic [physician responses were 41% less empathetic]. Even when physician responses were longer and scored higher for quality and empathy, they still remained significantly below chatbot scores. This shows implications for AI as a critical resource for fostering patient equity as individuals with mobility limitations or fear of medical bills are more likely to use messaging. However, this study did not assess neither the physician nor the bot response for fabricated information (Ayers 2023). [https://pubmed.ncbi.nlm.nih.gov/37115527/]\n",
    "\n",
    "**3. Conversational Artificial Intelligence for Spinal Pain Questionnaire**\n",
    "\n",
    "Efforts to apply artificial intelligence to medicine are actively underway with the creation and application of AI associated with natural language processing [NLU] However, unlike writing based chatbots like ChatGPT, Spoken Dialog Systems [SDS] are proving to be a possible gamechanger. SDS are able to communicate by voice and maintain conversation over long periods of time. Many companies already use SDS like Amazon with their Alexa, but this study tries to develop an SDS for a pain questionnaire for patients with spinal disease.\n",
    "\n",
    "The questionnaire was developed by inputting questions that medical staff usually ask during rounds of inpatients including questions about location, duration, intensity of the pain. Each question was structured in a closed format to allow for the system to easily answer questions. Real doctor-patient dialogue sets were collected for both the preoperative and postoperative pain questionnaires. \n",
    "\n",
    "The Breakdown:\n",
    "\n",
    "- The patient's voice is entered in a speech recognition module and converted into text data.\n",
    "- This data is entered into a natural language processing module and the output of the module is entered into a dialog management module which manages conversation flow\n",
    "- The dialogue module then searches for information to be given to the user which is generated as text data and then put back into the speech recognition module to be uttered in a coherent voice format. \n",
    "- Any input of patients that had to do with the expression of pain/mental state were classified [duration, timing, etc]. \n",
    "- Each patient’s personal information is assigned an ID number which allows the SDS to check if the patient already exists.\n",
    "- The SDS then makes a summary of the questionnaire and if proper information was not obtained, asks the question until the information is obtained. \n",
    "- When all the information is obtained, the SDS makes a summarized result statement of the questionnaire in text data, \n",
    "\n",
    "Overall, when evaluating the user satisfaction, the average score between doctors, nurses, and patients was 5.5 +- 1.4 / 7 points. There was higher satisfaction in regard to the clarity and positivity of the SDS, but relatively lower scores for the similarity of conversation with real people. In terms of the performance accuracy, the SDS struggled to recognize the answers of patient groups and increased repetition. In terms of errors, there were statistically different numbers of errors between doctors and patients and nurses, with nurses getting 0 errors. \n",
    "[https://doi-org.proxy.library.upenn.edu/10.14245/ns.2143080.540]\n",
    "\n",
    "**But, there are some drawbacks associated with this growth:**\n",
    "\n",
    "**A. Explainability and Interpretability**\n",
    "\n",
    "Explainability refers to the ability of models to justify results, while interpretability refers to the ability of the model to determine causes and effects from inputs and outputs. When considering explainability, models with high explanatory power are able to take into account multiple factors and use each factor to determine an outcome. As an example, when asked to predict when a person might die, a model would take into account factors like age, BMI, years spent smoking, and career and weigh each of those factors as more or less important in dictating a person’s lifespan. It allows models to know what inputs represent and how important they are to the overall outcome. When models fail to explain outcomes, it simply leads to a lack of accuracy in responses and model malfunctions. \n",
    "\n",
    "**B. Black Box Effect**\n",
    "\n",
    "Another thing to consider is the “black box” effect which refers to the lack of transparency of the operation of AI systems (Fenster 2023). Thus, the process in which AI models like ChatGPT analyzes is not fully understood by humans leading to undetected errors. Due to its tendency to hallucinate, unless experts in the field of question, many people would struggle to cross-check information provided due to the truthful manner it is provided in. This is especially true because unless prompted, it does not explain or advise on its limitations like the tendency to produce misinformation, bias in training data, or security risks from gathered data. This is a problem that lies mainly with experts designing these algorithms. The decisions for how models interact with users is left to the discretion of data scientists, leading to disconnects in customer desires for usage. Though these challenges are less applicable to medicine due to less data in large quantities especially when compared to fields like Bioinformatics. However, using databases like electronic health records (EHR) and other patient information databases not only bring up matters about data security but also the limited data may create records and solutions that fit modeling needs rather than medical needs (Vellido 2020). \n",
    "\n",
    "**C. Racial Bias**\n",
    "\n",
    "Another major issue lies in the clear clustering of health data available for white people when compared to people of color. This also extends to disparities in both gender and sexual orientation data. Discrimination is a major player in healthcare, and AI can mitigate health-care disparities, but at the same time, it can also exacerbate them. AI. Hospitals are becoming increasingly reliant on AI which is able to analyze and produce potential treatment options; however, there is a lack of consideration about algorithmic discrimination based on pre-existing biases and data in the medical field. Health data that is available can be limited/incorrect across people with different conditions and of different races which can lead to measurement errors and selection bias. It could also be historical patterns of discrimination which can lead to feedback loop bias. In 2019, a study was done to show how an algorithm, created by Optum, used by UnitedHealth Group was discriminating against Black patients. It was found that the algorithm was prioritizing care in hospitals based on healthcare costs and spending patterns, disproportionately targeting Black patients who historically have had lower health care spending. This is namely the only proven discrimination in healthcare, but instances like this are predicted to increase over time. The lack of guidelines and accountability for AI discrimination poses challenges for patients seeking legal recourse (Fenster 2023).\n",
    "\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "There is still much work to be done to fully understand and control the use of AI in healthcare. In its current form, ChatGPT and other LLMs can only serve as tools alongside knowledgeable healthcare professionals. Though they have already demonstrated their ability to bridge the gap between healthcare consumers and professionals in various ways, they lacked the ability to accurately interpret data a large percentage of the time. Though these applications are efficient, most are time consuming and few have been integrated into clinical practice. More comprehensive research needs to be conducted before the integration of AI in healthcare further.  Addressing these challenges through visualization and involving healthcare professionals' external assessment to ensure models address medical issues, comply with guidelines, and align with system-human interaction is the key to the integration of AI in healthcare (Vellido 2020).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e15ef2b-6d06-4193-9f95-34adff9c2465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3.11 (COMM4190)",
   "language": "python",
   "name": "comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
